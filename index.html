<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Michalis Vrigkas</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="canonical" href="https://mvrigkas.github.io">
		<link rel="stylesheet" href="assets/css/main.css" />
		 <link rel="stylesheet" href="assets/css/publications.css">
		<script type="text/javascript" src="assets/js/hidebib.js"></script>
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
        		
        <script type="text/javascript">

		  var _gaq = _gaq || [];
		  _gaq.push(['_setAccount', 'UA-25459109-1']);
		  _gaq.push(['_trackPageview']);
		
		  (function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		  })();
		
		</script>

	</head>
	<body>
   <!-- Nav -->
   <nav id="nav">
      <ul class="container">
         <li><a href="#top">Home</a></li>
        <li><a href="#publications">Publications</a></li>
        <li><a href="#downloads">Downloads</a></li>
        <li><a href="#contact">Contact</a></li>
     </ul>
   </nav>
   
   <!-- Home -->
   <div class="wrapper style1 first">
      <article class="container" id="top">
         <div class="row">
            <div class="4u 12u(mobile)">

               <span class="image fit"><img src="images/MVrigkas2.jpg" alt="" />Contact me:<br>m v r i g k a s /at\ u h /dot/ edu</span>

               <div class="row">
                  <div class="12u(mobile)">
                     <ul class="social">
                        
                     </ul>
                    
                  </div>
               </div>
            </div>
            <div class="8u 12u(mobile)">
               <header>
                  <h1 align="justify"><strong>Michalis Vrigkas, Ph.D.</strong></h1>
                  <!--<h3 align="justify">Postdoctoral Fellow, University of Houston, Texas, USA</h3>-->
               </header>
               <div id="myfont-size", align="justify">
					I am a postdoctoral fellow at the <a href="http://www.uh.edu" target="_blank">University of Houston</a> and member of the <a href="http://cbl.uh.edu" target="_blank">Computational Biomedicine Lab (CBL)</a>. I received my Ph.D. in 2016 from the <a href="http://www.cse.uoi.gr/en/" target="_blank">Department of Computer Science &amp; Engineering</a>, <a href="http://www.uoi.gr/en/" target="_blank">University of Ioannina</a>, Greece, under the supervision of Prof. <a href="http://www.cse.uoi.gr/~cnikou" target="_blank">Chistophoros Nikou</a>, and my M.Sc. and B.Sc. in <a href="http://www.cse.uoi.gr/en/" target="_blank">Computer Science</a> from the <a href="http://www.uoi.gr/en/" target="_blank">University of Ioannina</a>, Greece, in 2010 and 2008, respectively. My research is mainly focused on Image and Video Processing. It covers a wide range of topics such as Computer Vision, Image Analysis, Machine Learning, and Pattern Recognition with applications also to Medical Image Analysis, Biometrics, and Predictive Analytics.<br> <br>

				<div class="row">
                  <div class="12u(mobile)">
                     <ul class="social">
						<li><a href="Vrigkas_CV_en.pdf" class="icon fa-file-pdf-o"><span class="label">Pdf</span></a></li>
						<li><a href="https://www.linkedin.com/in/michalis-vrigkas-a3b09542" target="_blank" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
                        <li><a href="https://scholar.google.com/citations?user=hixpxHsAAAAJ&hl=en" target="_blank" class="ai ai-google-scholar-square"><span class="label"></span></a></li>
                        <li><a href="http://www.researchgate.net/profile/Michalis_Vrigkas" target="_blank" class="ai ai-researchgate-square"><span class="label"></span></a></li>
                        <!--<li><a href="http://uoi.academia.edu/MichalisVrigkas" target="_blank" class="ai ai-academia-square"><span class="label"></span></a></li>
                        <li><a href="https://github.com/mvrigkas" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>-->
					</ul>
				  </div>
				</div>
				  
               </div>
            </div>
         </div>
      </article>
   </div>
   <!-- Work -->
   <div class="wrapper style1">
      <article id="publications">
            <h2>Publications</h2>
         <div class="container">
            <div class="row">               
               <!--PUBLICATIONS--> 
               <div align="justify">				   
					<div id="publications-list">
						<ul class="list-unstyled">
						<!--2018--> 
							<li>
								<!--<div class="publication-inpress">2018 (In Press)</div>-->
								<div class="publication-year">2018</div>
								<div class="publication-title">SPICE: Superpixel classification for cell detection and counting</div>
								<div class="publication-authors">
									<span>O. Maga&ntilde;a-Tellez, <strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">International Conference on Computer Vision Theory and Applications</div>
								<div class="publication-type" id="VISAPP18">
									<span class="label label-danger"><a href="publications/Conferences/C10_VISAPP_2018_Funchal.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a href="javascript:toggleDisplay('VISAPP18abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<span class="label label-success"><a  href="javascript:togglebib('VISAPP18')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="VISAPP18abstract" class="span-boxcolor" >	
											An algorithm for the localization and counting of cells in histopathological images is presented. The algorithm relies on the presegmentation of an image into a number of superpixels followed by two random forests for classification. The first random forest determines if there are any cells in the superpixels at its input and the second random forest provides the number of cells in the respective superpixel. The algorithm is evaluated on a bone marrow histopathological dataset. We argue that a single random forest is not sufficient to detect all the cells in the image while a cascade of classifiers achieves higher accuracy. The results compare favorably with the state of the art but with a lower computational cost.
										</span>
									</span>									
									<!--<span class="label label-success"><a href="publications/Conferences/C10_VISAPP_2018_Funchal.bib">BibTex</a></span>-->
										<pre xml:space="preserve" >
@inproceedings{MOman_VISAPP18,
	author = {Oman Maga\~{n}a-Tellez and Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {SPICE: Superpixel classification for cell detection and counting},
	booktitle = {Proc. 13th International Conference on Computer Vision Theory and Applications},
	address = {Funchal, Madeira, Portugal},
	month = {January},
	pages = {485--490},
	year = {2018}
}									</pre>
								</div>
                            </li>
							
							<!--2017--> 
							<li>
								<div class="publication-year">2017</div>
								<div class="publication-title">Machine learning outperformed ACC/AHA Pooled Cohort Equations Risk Calculator for detection of high-risk asymptomatic individuals and recommending treatment for prevention of cardiovascular events in the Multi-Ethnic Study of Atherosclerosis (MESA)</div>
								<div class="publication-authors">
									<span>I.A. Kakadiaris, <strong>M. Vrigkas</strong>, M. Budoff, A. Yen, M. Naghavi</span>
								</div>
								<div class="publication-references">Circulation, American Heart Association, Scientific Sessions</div>
								<div class="publication-type" id="Circulation17">
									<!--<span class="label label-danger"><a href="publications/Abstracts/A1_AHA_2017.pdf">Article</a></span>&nbsp;&nbsp;&nbsp;	-->
									<span class="label label-primary"><a href="http://circ.ahajournals.org/content/136/Suppl_1/A23075" target="_blank">Extrernal link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('Circulation17abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Abstracts/A1_AHA_2017.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('Circulation17')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="Circulation17abstract" class="span-boxcolor" >	
										Studies have shown that the status quo for atherosclerotic cardiovascular disease (ASCVD) prediction in the U.S. - using ACC/AHA Pooled Cohort Equations Risk Calculator - is inaccurate and results in overtreatment of low-risk and undertreatment of high-risk individuals. Machine Learning (ML) is poised to revolutionize healthcare. We used ML to develop a new ASCVD risk calculator and tackled the problem.
										</span>
									</span>								
										<pre xml:space="preserve" >
@article{IKakadiaris_AHA17,
	author = {Ioannis Kakadiaris and Michalis Vrigkas and Matthew Budoff and Albert Yen and Morteza Naghavi},
	title = {Machine learning outperformed {ACC/AHA} {P}ooled {C}ohort {E}quations {R}isk {C}alculator for detection of high-risk asymptomatic individuals and recommending treatment for prevention of cardiovascular events in the {M}ulti-{E}thnic {S}tudy of {A}therosclerosis {(MESA)}},
	volume = {136},
	number = {Suppl 1},
	pages = {A23075--A23075},
	year = {2017},
	month = {November 11-15},
	address = {Anaheim, CA},
	publisher = {American Heart Association, Inc.},
	issn = {0009-7322},
	URL = {http://circ.ahajournals.org/content/136/Suppl_1/A23075},
	eprint = {http://circ.ahajournals.org/content},
	journal = {Circulation}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2017</div>
								<div class="publication-title">Human activity recognition using robust adaptive privileged probabilistic learning</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, E. Kazakos, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">arXiv preprint arXiv:1709.06447</div>
								<div class="publication-type" id="CVIU18">
									<span class="label label-danger"><a href="publications/Journals/J6_Vrigkas_arxiv_2017.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="https://arxiv.org/abs/1709.06447" target="_blank">External link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('CVIU18abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J6_Vrigkas_arxiv_2017.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('CVIU18')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="CVIU18abstract" class="span-boxcolor" >	
										In this work, a novel method based on the learning using privileged information (LUPI) paradigm for recognizing complex human activities is proposed that handles missing information during testing. We present a supervised probabilistic approach that integrates LUPI into a hidden conditional random field (HCRF) model. The proposed model is called HCRF+ and may be trained using both maximum likelihood and maximum margin approaches. It employs a self-training technique for automatic estimation of the regularization parameters of the objective functions. Moreover, the method provides robustness to outliers (such as noise or missing data) by modeling the conditional distribution of the privileged information by a Student's <italic>t</italic>-density function, which is naturally integrated into the HCRF+ framework. Different forms of privileged information were investigated. The proposed method was evaluated using four challenging publicly available datasets and the experimental results demonstrate its effectiveness with respect to the-state-of-the-art in the LUPI framework using both hand-crafted features and features extracted from a convolutional neural network.
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_arxiv17,
	title={Human activity recognition using robust adaptive privileged probabilistic learning},
	author={Michalis Vrigkas and Evangelos Kazakos and Christophoros Nikou and Ioannis A. Kakadiaris},
	journal={arXiv preprint arXiv:1709.06447},
	year={2017}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2017</div>
								<div class="publication-title">Inferring human activities using robust privileged probabilistic learning</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, E. Kazakos, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">International Conference on Computer Vision Workshops</div>
								<div class="publication-type" id="ICCVW17_2">
									<span class="label label-danger"><a href="publications/Conferences/C9_ICCVW_2017_2_Venice.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICCVW17_2abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C9_ICCVW_2017_2_Venice.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICCVW17_2')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICCVW17_2abstract" class="span-boxcolor" >	
										Classification models may often suffer from &ldquo;structure imbalance&rdquo; between training and testing data that may occur due to the deficient data collection process. This imbalance can be represented by the learning using privileged information (LUPI) paradigm. In this paper, we present a supervised probabilistic classification approach that integrates LUPI into a hidden conditional random field (HCRF) model. The proposed model is called LUPI-HCRF and is able to cope with additional information that is only available during training. Moreover, the proposed method employes Student’s <italic>t</italic>-distribution to provide robustness to outliers by modeling the conditional distribution of the privileged information. Experimental results in three publicly available datasets demonstrate the effectiveness of the proposed approach and improve the state-of-the-art in the LUPI framework for recognizing human activities.
										</span>
									</span>								
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_ICCVW17,
	author = {Michalis Vrigkas and Evangelos Kazakos and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Inferring human activities using robust privileged probabilistic learning},
	booktitle = {Proc. IEEE International Conference on Computer Vision Workshops},
	year = {2017},
	month = {October},
	pages = {2658--2665},
	address = {Venice, Italy}
}									</pre>
								</div>
                            </li>
														
							<li>
								<div class="publication-year">2017</div>
								<div class="publication-title">Adaptive SVM+: Learning with privileged information for domain adaptation</div>
								<div class="publication-authors">
									<span>N. Sarafianos, <strong>M. Vrigkas</strong>, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">International Conference on Computer Vision Workshops</div>
								<div class="publication-type" id="ICCVW17">
									<span class="label label-danger"><a href="publications/Conferences/C8_ICCVW_2017_1_Venice.pdf">Article</a></span><span style="margin-right:15px"></span>			
									<span class="label label-info"><a href="publications/Conferences/C8_ICCVW_2017_1_Suppl_Venice.pdf">Supplementary</a></span><span style="margin-right:15px"></span>		
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICCVW17abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>						
									<!--<span class="label label-success"><a href="publications/Conferences/C8_ICCVW_2017_1_Venice.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICCVW17')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICCVW17abstract" class="span-boxcolor" >	
										Incorporating additional knowledge in the learning process can be beneficial for several computer vision and machine learning tasks. Whether privileged information originates from a source domain that is adapted to a target domain, or as additional features available at training time only, using such privileged (i.e., auxiliary) information is of high importance as it improves the recognition performance and generalization. However, both primary and privileged information are rarely derived from the same distribution, which poses an additional challenge to the recognition task. To address these challenges, we present a novel learning paradigm that leverages privileged information in a domain adaptation setup to perform visual recognition tasks. The proposed framework, named Adaptive SVM+, combines the advantages of both the learning using privileged information (LUPI) paradigm and the domain adaptation framework, which are naturally embedded in the objective function of a regular SVM. We demonstrate the effectiveness of our approach on the publicly available Animals with Attributes and INTERACT datasets and report state-of-the-art results in both of them.
										</span>
									</span>					
										<pre xml:space="preserve" >
@inproceedings{NSarafianos_ICCVW17,
	author = {Nikolaos Sarafianos and Michalis Vrigkas and Ioannis A. Kakadiaris},
	title = {Adaptive SVM+: Learning with privileged information for domain adaptation},
	booktitle = {Proc. IEEE International Conference on Computer Vision Workshops},
	year = {2017},
	month = {October},
	pages = {2637--2644},
	address = {Venice, Italy}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2017</div>
								<div class="publication-title">Identifying human behaviors using synchronized audio-visual cues</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">IEEE Transactions on Affective Computing</div>
								<div class="publication-type" id="TAffC17">
									<span class="label label-danger"><a href="publications/Journals/J5_Vrigkas_IEEE_TAFFC_2017.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7350122&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7350122" target="_blank">Extrernal link</a></span>&nbsp;&nbsp;&nbsp;
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('TAffC17abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J5_Vrigkas_IEEE_TAFFC_2017.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('TAffC17')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="TAffC17abstract" class="span-boxcolor" >	
										In this paper, a human behavior recognition method using multimodal features is presented. We focus on modeling individual and social behaviors of a subject (e.g., friendly/aggressive or hugging/kissing behaviors) with a hidden conditional random field (HCRF) in a supervised framework. Each video is represented by a vector of spatio-temporal visual features (STIP, head orientation and proxemic features) along with audio features (MFCCs). We propose a feature pruning method for removing irrelevant and redundant features based on the spatio-temporal neighborhood of each feature in a video sequence. The proposed framework assumes that human movements are highly correlated with sound emissions. For this reason, canonical correlation analysis (CCA) is employed to find correlation between the audio and video features prior to fusion. The experimental results, performed in two human behavior recognition datasets including political speeches and human interactions from TV shows, attest the advantages of the proposed method compared with several baseline and alternative human behavior recognition methods.
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_TAffC15,
	author = {Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Identifying human behaviors using synchronized audio-visual cues},
	journal = {IEEE Transactions on Affective Computing},
	year = {2017},
	volume={8}, 
	number={1}, 
	pages={54-66},
	doi={10.1109/TAFFC.2015.2507168},
	month={January}
}									</pre>
								</div>
                            </li>
							
							<!--2016--> 
							<li>
								<div class="publication-year">2016</div>
								<div class="publication-title">Active privileged learning of human activities from weakly labeled samples</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">IEEE International Conference on Image Processing</div>
								<div class="publication-type" id="ICIP16">
									<span class="label label-danger"><a href="publications/Conferences/C7_ICIP_2016_Phoenix.pdf">Article</a></span><span style="margin-right:15px"></span>	
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICIP16abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C7_ICIP_2016_Phoenix.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICIP16')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICIP16abstract" class="span-boxcolor" >	
										In many human activity recognition systems the size of the unlabeled training data may be significantly large due to expensive human effort required for data annotation. Moreover, the insufficient data collection process from heterogenous sources may cause dissimilarities between training and testing data. To address these limitations, a novel probabilistic approach that combines learning using privileged information (LUPI) and active learning is proposed. A pool-based privileged active learning approach is presented for semi-supervising learning of human activities from multimodal labeled and unlabeled data. Both uncertainty and distance from the decision boundary are used as a query inference strategies for selecting an unlabeled observation and query its label. Experimental results in four publicly available datasets demonstrate that the proposed method can identify with high accuracy complex human activities.
										</span>
									</span>								
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_ICIP16,
	author = {Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Active privileged learning of human activities from weakly labeled samples},
	booktitle = {Proc. 23rd IEEE International Conference on Image Processing},
	year = {2016},
	month = {September},
	pages = {3036--3040},
	address = {Phoenix, AZ}
}									</pre>
								</div>
                            </li>
																					
							<li>
								<div class="publication-year">2016</div>
								<div class="publication-title">Exploiting privileged information for facial expression recognition<span style="margin-right:15px"></span><span class="label label-warning">Honorable Mention Paper Award</span></div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">IAPR/IEEE International Conference on Biometrics</div>
								<div class="publication-type" id="ICB16">
									<span class="label label-danger"><a href="publications/Conferences/C6_ICB_2016_Halmstad.pdf">Article</a></span><span style="margin-right:15px"></span>	
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICB16abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C6_ICB_2016_Halmstad.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICB16')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICB16abstract" class="span-boxcolor" >	
										Most of the facial expression recognition methods consider that both training and testing data are equally distributed. As facial image sequences may contain information for heterogeneous sources, facial data may be asymmetrically distributed between training and testing, as it may be difficult to maintain the same quality and quantity of information. In this work, we present a novel classification method based on the learning using privileged information (LUPI) paradigm to address the problem of facial expression recognition. We introduce a probabilistic classification approach based on conditional random fields (CRFs) to indirectly propagate knowledge from privileged to regular feature space. Each feature space owns specific parameter settings, which are combined together through a Gaussian prior, to train the proposed t-CRF+ model and allow the different tasks to share parameters and improve classification performance. The proposed method is validated on two challenging and publicly available benchmarks on facial expression recognition and improved the state-of-theart methods in the LUPI framework.
										</span>
									</span>								
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_ICB16,
	author = {Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Exploiting privileged information for facial expression recognition},
	booktitle = {Proc. 9th IAPR/IEEE International Conference on Biometrics},
	year = {2016},
	month = {June},
	pages = {1--8},
	address = {Halmstad, Sweden},
	doi={10.1109/ICB.2016.7550048},
	note = {Honorable Mention Paper Award}
}									</pre>
									
								</div>
                            </li>
							
							<!--2015--> 
							<li>
								<div class="publication-year">2015</div>
								<div class="publication-title">A review of human activity recognition methods</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">Frontiers in Robotics and Artificial Intelligence</div>
								<div class="publication-type" id="FRONTIERS15">
									<span class="label label-danger"><a href="publications/Journals/J4_Vrigkas_FRONTIERS_2015.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="http://journal.frontiersin.org/article/10.3389/frobt.2015.00028/abstract" target="_blank">Extrernal link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('FRONTIERS15abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J4_Vrigkas_FRONTIERS_2015.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('FRONTIERS15')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="FRONTIERS15abstract" class="span-boxcolor" >	
										Recognizing human activities from video sequences or still images is a challenging task due to problems, such as background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance. Many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system. In this work, we provide a detailed review of recent and state-of-the-art research advances in the field of human activity classification. We propose a categorization of human activity methodologies and discuss their advantages and limitations. In particular, we divide human activity classification methods into two large categories according to whether they use data from different modalities or not. Then, each of these categories is further analyzed into sub-categories, which reflect how they model human activities and what type of activities they are interested in. Moreover, we provide a comprehensive analysis of the existing, publicly available human activity classification datasets and examine the requirements for an ideal human activity recognition dataset. Finally, we report the characteristics of future research directions and present some open issues on human activity recognition.
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_FRONTIERS2015,
	author = {Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {A review of human activity recognition methods},
	journal = {Frontiers in Robotics and Artificieal Inteligence},
	volume = {2},
	number = {28},
	pages = {1--26},
	year = {2015},
	url={http://www.frontiersin.org/vision_systems_theory,_tools_and_applications/10.3389/frobt.2015.00028/abstract},
	doi={10.3389/frobt.2015.00028},
	issn={2296-9144}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2015</div>
								<div class="publication-title">Segmentation of cell clusters in Pap smear images using intensity variation between superpixels</div>
								<div class="publication-authors">
									<span>M.E. Plissiti, <strong>M. Vrigkas</strong>, C. Nikou</span>
								</div>
								<div class="publication-references">International Conference on Systems, Signals and Image Processing</div>
								<div class="publication-type" id="IWSSIP15">
									<span class="label label-danger"><a href="publications/Conferences/C5_IWSSIP_London_2015.pdf">Article</a></span><span style="margin-right:15px"></span>	
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('IWSSIP15abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C5_Plisiti_IWSSIP_2015.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('IWSSIP15')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="IWSSIP15abstract" class="span-boxcolor" >	
										The automated interpretation of Pap smear images is a challenging issue with several aspects. The accurate segmentation of the structuring elements of each cell is a crucial procedure which entails in the correct identification of pathological situations. However, the extended cell overlapping in Pap smear slides complicates the automated analysis of these cytological images. In this work, we propose an efficient algorithm for the separation of the cytoplasm area of overlapping cells. The proposed method is based on the fact that in isolated cells the pixels of the cytoplasm exhibit similar features and the cytoplasm area is homogeneous. Thus, the observation of intensity changes in extended subareas of the cytoplasm indicates the existence of overlapping cells. In the first step of the proposed method, the image is tesselated into perceptually meaningful individual regions using a superpixel algorithm. In a second step, these areas are merged into regions exhibiting the same characteristics, resulting in the identification of each cytoplasm area and the corresponding nuclei. The area of overlap is then detected using an algorithm that specifies faint changes in the intensity of the cytoplasm of each cell. The method has been evaluated on cytological images of conventional Pap smears, and the results are very promising.
										</span>
									</span>							
										<pre xml:space="preserve" >
@inproceedings{MPlissiti_IWSSIP15,
	author = {Marina E. Plissiti and Michalis Vrigkas and Christophoros Nikou},
	title = {Segmentation of cell clusters in Pap smear images using intensity variation between superpixels},
	booktitle = {Proc. 22nd International Conference on Systems, Signals and Image Processing},
	year = {2015},
	month = {September},
	pages = {184--187},
	address = {London, UK}
}									</pre>
								</div>
                            </li>
							
							<!--2014--> 
							<li>
								<div class="publication-year">2014</div>
								<div class="publication-title">Robust maximum a posteriori image super-resolution</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, L.P. Kondi</span>
								</div>
								<div class="publication-references" id="JEI14a">Journal of Electronic Imaging</div>
								<div class="publication-type" id="JEI14">
									<span class="label label-danger"><a href="publications/Journals/J3_Vrigkas_JEI_2014.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="http://dx.doi.org/10.1117/1.JEI.23.4.043016" target="_blank">Extrernal link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('JEI14abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J3_Vrigkas_JEI_2014.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('JEI14')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="JEI14abstract" class="span-boxcolor" >	
										A global robust M-estimation scheme for maximum a posteriori (MAP) image super-resolution which efficiently addresses the presence of outliers in the low-resolution images is proposed. In iterative MAP image super-resolution, the objective function to be minimized involves the highly resolved image, a parameter controlling the step size of the iterative algorithm, and a parameter weighing the data fidelity term with respect to the smoothness term. Apart from the robust estimation of the high-resolution image, the contribution of the proposed method is twofold: (1) the robust computation of the regularization parameters controlling the relative strength of the prior with respect to the data fidelity term and (2) the robust estimation of the optimal step size in the update of the high-resolution image. Experimental results demonstrate that integrating these estimations into a robust framework leads to significant improvement in the accuracy of the high-resolution image
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_JEI14,
	author = {Michalis Vrigkas and Christophoros Nikou and Lisimachos P. Kondi},
	title = {Robust maximum a posteriori image super-resolution},
	journal = {Journal of Electronic Imaging},
	volume = {23},
	number = {4},
	pages = {043016},
	year = {2014},
	isbn = {1017-9909},
	doi = {10.1117/1.JEI.23.4.043016},
	URL = {http://dx.doi.org/10.1117/1.JEI.23.4.043016}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2014</div>
								<div class="publication-title">Classifying behavioral attributes using conditional random fields</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">Hellenic Conference on Artificial Intelligence</div>
								<div class="publication-type" id="SETN14">
									<span class="label label-danger"><a href="publications/Conferences/C4_Vrigkas_SETN_Ioannina_2014.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('SETN14abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C4_Vrigkas_SETN_2014.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('SETN14')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="SETN14abstract" class="span-boxcolor" >	
										A human behavior recognition method with an application to political speech videos is presented. We focus on modeling the behavior of a subject with a conditional random field (CRF). The unary terms of the CRF employ spatiotemporal features (i.e., HOG3D, STIP and LBP). The pairwise terms are based on kinematic features such as the velocity and the acceleration of the subject. As an exact solution to the maximization of the posterior probability of the labels is generally intractable, loopy belief propagation was employed as an approximate inference method. To evaluate the performance of the model, we also introduce a novel behavior dataset, which includes low resolution video sequences depicting different people speaking in the Greek parliament. The subjects of the Parliament dataset are labeled as friendly, aggressive or neutral depending on the intensity of their political speech. The discrimination between friendly and aggressive labels is not straightforward in political speeches as the subjects perform similar movements in both cases. Experimental results show that the model can reach high accuracy in this relatively difficult dataset.
										</span>
									</span>								
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_SETN14,
	author = {Michalis Vrigkas and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Classifying behavioral attributes using conditional random fields},
	booktitle = {Proc. 8th Hellenic Conference on Artificial Intelligence},
	year = {2014},
	month = {May},
	pages = {95--104},
	volume={8445},
	series={Lecture Notes in Computer Science},
	address = {Ioannina, Greece}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2014</div>
								<div class="publication-title">Matching mixtures of curves for human action recognition</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, V. Karavasilis, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">Computer Vision and Image Understanding</div>
								<div class="publication-type" id="CVIU14">
									<span class="label label-danger"><a href="publications/Journals/J2_Vrigkas_CVIU_2014.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="http://dx.doi.org/10.1016/j.cviu.2013.11.007" target="_blank">Extrernal link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('CVIU14abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J2_Vrigkas_CVIU_2014.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('CVIU14')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="CVIU14abstract" class="span-boxcolor" >	
										A learning-based framework for action representation and recognition relying on the description of an action by time series of optical flow motion features is presented. In the learning step, the motion curves representing each action are clustered using Gaussian mixture modeling (GMM). In the recognition step, the optical flow curves of a probe sequence are also clustered using a GMM, then each probe sequence is projected onto the training space and the probe curves are matched to the learned curves using a non-metric similarity function based on the longest common subsequence, which is robust to noise and provides an intuitive notion of similarity between curves. Alignment between the mean curves is performed using canonical time warping. Finally, the probe sequence is categorized to the learned action with the maximum similarity using a nearest neighbor classification scheme. We also present a variant of the method where the length of the time series is reduced by dimensionality reduction in both training and test phases, in order to smooth out the outliers, which are common in these type of sequences. Experimental results on KTH, UCF Sports and UCF YouTube action databases demonstrate the effectiveness of the proposed method.
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_CVIU14,
	author = {Michalis Vrigkas and Vasileios Karavasilis and Christophoros Nikou and Ioannis A. Kakadiaris},
	title = {Matching mixtures of curves for human action recognition},
	journal = {Computer Vision and Image Understandin}",
	volume = {119},
	pages = {27--4}",
	year = {2014},
	issn = {1077--3142},
	doi = {http://dx.doi.org/10.1016/j.cviu.2013.11.007}
}									</pre>
								</div>
                            </li>
							
							<!--2013--> 
							<li>
								<div class="publication-year">2013</div>
								<div class="publication-title">Accurate image registration for MAP image super-resolution</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, L.P. Kondi</span>
								</div>
								<div class="publication-references">Signal Processing: Image Communication</div>
								<div class="publication-type" id="SPIC13">
									<span class="label label-danger"><a href="publications/Journals/J1_Vrigkas_SPIC_2013.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-primary"><a href="http://dx.doi.org/10.1016/j.image.2012.12.008" target="_blank">Extrernal link</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('SPIC13abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Journals/J1_Vrigkas_SPIC_2013.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('SPIC13')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="SPIC13abstract" class="span-boxcolor" >	
										The accuracy of image registration plays a dominant role in image super-resolution methods and in the related literature, landmark-based registration methods have gained increasing acceptance in this framework. In this work, we take advantage of a maximum a posteriori (MAP) scheme for image super-resolution in conjunction with the maximization of mutual information to improve image registration for super-resolution imaging. Local as well as global motion in the low-resolution images is considered. The overall scheme consists of two steps. At first, the low-resolution images are registered by establishing correspondences between image features. The second step is to fine-tune the registration parameters along with the high-resolution image estimation, using the maximization of mutual information criterion. Quantitative and qualitative results are reported indicating the effectiveness of the proposed scheme, which is evaluated with different image features and MAP image super-resolution computation methods.
										</span>
									</span>
										<pre xml:space="preserve" >
@article{MVrigkas_SPIC13,
	author = {Michalis Vrigkas and Christophoros Nikou and Lisimachos P. Kondi},
	title = {Accurate image registration for \{MAP\} image super-resolution},
	journal = {Signal Processing: Image Communication},
	volume = {28},
	number = {5},
	pages = {494--508},
	year = {2013},
	issn = {0923-5965},
	doi = {10.1016/j.image.2012.12.008}
}									</pre>
								</div>
                            </li>
							
							<li>
								<div class="publication-year">2013</div>
								<div class="publication-title">Action recognition by matching clustered trajectories of motion vectors</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, V. Karavasilis, C. Nikou, I.A. Kakadiaris</span>
								</div>
								<div class="publication-references">International Conference on Computer Vision Theory and Applications</div>
								<div class="publication-type" id="VISAPP13">
									<span class="label label-danger"><a href="publications/Conferences/C3_VISAPP_Barcelona_2013.pdf">Article</a></span><span style="margin-right:15px"></span>	
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('VISAPP13abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C3_Vrigkas_VISAPP_2013.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('VISAPP13')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="VISAPP13abstract" class="span-boxcolor" >	
										A framework for action representation and recognition based on the description of an action by time series of optical flow motion features is presented. In the learning step, the motion curves representing each action are clustered using Gaussian mixture modeling (GMM). In the recognition step, the optical flow curves of a probe sequence are also clustered using a GMM and the probe curves are matched to the learned curves using a non-metric similarity function based on the longest common subsequence which is robust to noise and provides an intuitive notion of similarity between trajectories. Finally, the probe sequence is categorized to the learned action with the maximum similarity using a nearest neighbor classification scheme. Experimental results on common action databases demonstrate the effectiveness of the proposed method.
										</span>
									</span>							
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_VISAPP13,
	author = {Michalis Vrigkas and Vasileios Karavasilis and Christophoros Nikou and Ioannis Kakadiaris},
	title = {Action recognition by matching clustered trajectories of motion vectors},
	booktitle = {Proc. 8th International Conference on Computer Vision Theory and Applications},
	year = {2013},
	pages = {112--117},
	address = {Barcelona, Spain},
	month = {February}
}									</pre>
								</div>
                            </li>	
							
							<!--2012--> 
							<li>
								<div class="publication-year">2012</div>
								<div class="publication-title">A fully robust framework for MAP image super-resolution</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, L.P. Kondi</span>
								</div>
								<div class="publication-references">IEEE International Conference on Image Processing</div>
								<div class="publication-type" id="ICIP12">
									<span class="label label-danger"><a href="publications/Conferences/C2_IEEE_ICIP-Orlado-2012.pdf">Article</a></span><span style="margin-right:15px"></span>
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICIP12abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C2_Vrigkas_ICIP_2012.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICIP12')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICIP12abstract" class="span-boxcolor" >	
										In this work, we propose an adaptive M-estimation scheme for robust image super-resolution. The proposed algorithm relies on a maximum <italic>a posteriori</italic> (MAP) framework and addresses the presence of outliers in the low resolution images. Moreover, apart from the robust estimation of the high resolution image, the contribution of the method is twofold: (i) the robust computation of the regularization parameters controlling the relative strength of the prior with respect to the data fidelity term and (ii) the robust estimation of the optimal step size in the update of the high resolution image. Experimental results demonstrate that integrating these estimations into a robust framework leads to significant improvement in the accuracy of the high resolution image.
										</span>
									</span>									
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_ICIP12,
  author = {Michalis Vrigkas and Christophoros Nikou and Lisimachos P. Kondi},
  title = {A fully robust framework for MAP image Super-Resolution},
  booktitle = {Proc. IEEE International Conference on Image Processing},
  year = {2012},
  pages = {2225--2228},
  address = {Orlando, FL},
  month = {September}
}									</pre>
								</div>
                            </li>
							
							<!--2011--> 
							<li>
								<div class="publication-year">2011</div>
								<div class="publication-title">On the improvement of image registration for high accuracy super-resolution</div>
								<div class="publication-authors">
									<span><strong>M. Vrigkas</strong>, C. Nikou, L.P. Kondi</span>
								</div>
								<div class="publication-references">IEEE International Conference on Acoustics, Speech and Signal Processing</div>
								<div class="publication-type" id="ICASSP11">
									<span class="label label-danger"><a href="publications/Conferences/C1_IEEE_ICASSP-Prague-2011.pdf">Article</a></span><span style="margin-right:15px"></span>		
									<span class="label label-abstract"> <a  href="javascript:toggleDisplay('ICASSP11abstract',this)">Abstract</a></span><span style="margin-right:15px"></span>
									<!--<span class="label label-success"><a href="publications/Conferences/C1_Vrigkas_ICASSP_2011.bib">BibTex</a></span>-->
									<span class="label label-success"><a  href="javascript:togglebib('ICASSP11')" class="togglebib" >BibTex</a></span>
									<span class="elaboration"> 
										<span id="ICASSP11abstract" class="span-boxcolor" >	
										Accurate image registration plays a preponderant role in image super-resolution methods and in the related literature landmarkbased registration methods have gained increasing acceptance in this framework. However, their solution relies on point correspondences and on least squares estimation of the registration parameters necessitating further improvement. In this work, a maximum a posteriori scheme for image super-resolution is presented where the image registration part is accomplished in two steps. At first, the lowresolution images are registered by establishing correspondences between robust SIFT features. In the second step, the estimation of the registration parameters is fine-tuned along with the estimation of the high resolution image, in an iterative scheme, using the maximization of the mutual information criterion. Numerical results showed that the reconstructed image is consistently of higher quality than in standard MAP-based methods employing only landmarks.
										</span>
									</span>						
										<pre xml:space="preserve" >
@inproceedings{MVrigkas_ICASSP11,
  author = {Michalis Vrigkas and Christophoros Nikou and Lisimachos P. Kondi},
  title =  {On the improvement of image registration for high accuracy super-resolution},
  booktitle = {Proc. IEEE International Conference on Acoustics, Speech and Signal Processing},
  year = {2011},
  pages = {981--984},
  address = {Prague, Czech Republic},
  month = {May}
}									</pre>
								</div>
                            </li>
							
						</ul>
					</div>               									

			<div id="myfont-size", align="justify">
                  	   <blockquote><!--IEEE Copyright Notice: -->This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. These works may not be reposted without the explicit permission of the copyright holder.</blockquote>
                  	   <!--<blockquote>ACM Copyright Notice: Permission to make digital or hard copies of part or all of all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept., ACM, Inc., fax +1 (212) 869-0481, or permissions@acm.org.</blockquote>-->
	               	</div>
               </div>               
               
            </div>
         </div>
      </article>
   </div>
   <!-- Downloads -->
   <div class="wrapper style2">
      <article id="downloads">
         <header>
            <h2>Download data/software</h2>
         </header>
         <div class="container">
            <div class="row">
               <div class="4u 12u(mobile)">
                  <article class="box style2">
                     <a href="#downloads" class="image featured"><img src="images/parliament.jpg" alt="" /></a>
                     <h3><a href="http://www.cse.uoi.gr/~mvrigkas/ParliamentDataset/Parliament_Dataset.zip">The Parliament dataset</a></h3>
                     <p>You can donwload the dataset <a href="http://www.cse.uoi.gr/~mvrigkas/ParliamentDataset/Parliament_Dataset.zip">here</a>.</p>
                  </article>
               </div>
               <div class="8u 12u(mobile)">
                  <div id="myfont-size", align="justify">&nbsp;&nbsp;&nbsp;The Parliament dataset is a collection of 228 video sequences, depicting political speeches in the Greek parliament, at a resolution of 320 Ã— 240 pixels at 25 fps. All behaviors were recorded for 20 different subjects. The videos were acquired with a static camera and contain uncluttered backgrounds. The length of the video sequences is 250 frames. The video sequences were manually labeled with one of three behavioral labels: friendly (90 videos), aggressive (73 videos), or neutral (65 videos). The subjects express their opinion on a specific law proposal and they adjust their body movements and voice intensity level according to whether they agree with that or not.<br>
                     &nbsp;&nbsp;&nbsp;The dataset was annotated by two observers of Greek origin, who watched the videos independently and recorded their labels separately. Disagreement was resolved by a third observer. The observers were asked to categorize the videos with respect to the notions of kindness and aggressiveness according to a general perception of a political speech by a citizen with a Greek mentality as follows. (i) Subjects with large and abrupt body, head and hand movements and high speech signal amplitude are to be labeled as aggressive. This corresponds to statesmen who express strongly their disagreement with the topic discussed or a previous speech given by a political opponent. (ii) Subjects with very small variations in their motion and speech signal amplitude are to be labeled as neutral. This class includes standard political speeches only expressing a point of view without any strong indication (body motion or voice tone) of agreement or disagreement with the topic discussed. (iii) Subjects with large but smooth variations in the pose of their body and hands speaking with a normal speech signal amplitudes are to be labeled as friendly.<br>
                     
                     &nbsp;&nbsp;&nbsp;If you use this dataset, I would be grateful if you cite with one of the following related publications:
                  </div>
                  <br>
                  <h3 align="left">Related Publications</h3>
                  <div id="myfont-size", align="justify">
                     
                        <!--<ol type="1" reversed>-->
                        <ol type="1">
						  <li>&nbsp;<strong>M. Vrigkas</strong>, E. Kazakos, C. Nikou and I. A. Kakadiaris, &ldquo;<strong>Human activity recognition using robust adaptive privileged probabilistic learning</strong>,&rdquo; arXiv preprint arXiv:1709.06447, 2017. [<a href="publications/Journals/J6_Vrigkas_arxiv_2017.pdf">pdf</a>] [<a href="https://arxiv.org/abs/1709.06447" target="_blank">arXiv</a>] [<a href="publications/Journals/J6_Vrigkas_arxiv_2017.bib">bibtex</a>]</li>
						  
						  <li>&nbsp;<strong>M. Vrigkas</strong>, E. Kazakos, C. Nikou and I. A. Kakadiaris, &ldquo;<strong>Inferring human activities using robust privileged probabilistic learning</strong>,&rdquo; in Proc. IEEE International Conference on Computer Vision Workshops, pp. 2658-2665, Venice, Italy, October 22-29 2017. [<a href="publications/Conferences/C9_ICCVW_2017_2_Venice.pdf">pdf</a>] [<a href="publications/Conferences/C9_ICCVW_2017_2_Venice.bib">bibtex</a>]</li>
						  
						  <li>&nbsp;<strong>M. Vrigkas</strong>, C. Nikou and I.A. Kakadiaris, &ldquo;<strong>Identifying human behaviors using synchronized audio-visual cues</strong>,&rdquo; IEEE Transactions on Affective Computing, vol. 8, no. 1, pp. 54-66, Jan.-March, 2017. [<a href="publications/Journals/J5_Vrigkas_IEEE_TAFFC_2017.pdf">pdf</a>] [<a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7350122&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7350122" target="_blank">Extrernal link</a>] [<a href="publications/Journals/J5_Vrigkas_IEEE_TAFFC_2017.bib">bibtex</a>]</li>
						  
                       	  <li>&nbsp;<strong>M. Vrigkas</strong>, C. Nikou and I.A. Kakadiaris, &ldquo;<strong>Classifying behavioral attributes using conditional random fields</strong>,&rdquo; in Proc. 8th Hellenic Conference on Artificial Intelligence, Lecture Notes in Computer Science, vol. 8445, pp. 95-104, Ioannina, Greece,  May 15-17 2014. [<a href="publications/Conferences/C4_Vrigkas_SETN_Ioannina_2014.pdf">pdf</a>] [<a href="publications/Conferences/C4_Vrigkas_SETN_2014.bib">bibtex</a>]</li>
                        </ol>
                  </div>
               </div>
            </div>
         </div>
      </article>
   </div>
   
   <!-- Contact -->
   <div class="wrapper style4">
      <article id="contact">
         <header>
            <h2>Contact Information</h2>
            <p>Feel free to communicate with me for any suggestions or comments about my work:</p>
            
         </header>
         <div class="container">
            <div class="row">
               <div class="6u 12u(mobile)">
                  <div align="left">
                     		
						<h3>Michalis Vrigkas</h3>
                        Computational Biomedicine Lab<br>
                        University of Houston<br>
                        Room HBS 326, Health and Biomedical Sciences Center<br>
                        4849 Calhoun Rd., Houston, TX 77204, USA<br>
                        Web: <a href="https://mvrigkas.github.io/">https://mvrigkas.github.io/</a><br>
                        Email: m v r i g k a s /at\ u h /dot/ edu (properly processed)
                     </div>
                  </div>
                  <div class="2u 12u(mobile)">
                  </div>
                  <div class="4u 12u(mobile)">
                     <div><span class="image fit"><iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d2086.849184969082!2d-95.34019504189428!3d29.716839699775175!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x8640be5ad7b5a945%3A0xb85e2a0e84f44c78!2sHealth+and+Biomedical+Sciences+Center!5e1!3m2!1sen!2sgr!4v1448816207253" height="240" width="100% frameborder="0" style="border:0"></iframe></span></div>
                  </div>

            </div>
         </div>
         
      </article>
   </div>
   <!-- Scripts -->
   <script src="assets/js/jquery.min.js"></script>
   <script src="assets/js/jquery.scrolly.min.js"></script>
   <script src="assets/js/skel.min.js"></script>
   <script src="assets/js/skel-viewport.min.js"></script>
   <script src="assets/js/util.js"></script>
   <!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
   <script src="assets/js/main.js"></script>   
   <script xml:space="preserve" language="JavaScript">hideallbibs();</script>
   <script xml:space="preserve" language="JavaScript">hideallabstracts();</script>
</body>
</html>
